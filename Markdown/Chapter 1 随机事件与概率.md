1 随机事件与概率

## 1.1 随机事件及其运算

### 1.1.1 随机事件与随机试验

​		随机事件就是可能发生也可能不发生的事件。例如：

* 认认真真看完这个教程。

* 实现一个小目标，挣他一个亿。

* 看完《百年孤独》。

  这些事件都是可能发生也可能不发生的，因此都是随机事件。

  随机试验则是一个过程。它满足这样一些条件：

1. 试验结果是确定的。
2. 试验究竟会出现什么结果是随机的。
3. 每次试验仅仅有唯一的结果。

例如：抛一枚硬币，考虑硬币朝上的一面，实验前你不知道究竟会出现什么结果，数字或者花或者立起来；试验结束后仅仅出现一种结果，朝上的一面要么是数字，要么是花，要么立起来。因此抛硬币就是一个随机试验。

​	一种特定的实验结果叫做一个样本点，所有的样本点构成了样本空间。

例如：上面提到的抛硬币实验，数字朝上、花朝上、立起来分别是三个样本点。他们的集合$S$构成了样本空间。
$$
S=\{数字面，花面，立起来\}
$$




### 1.1.2 事件间的关系和运算

​	事件之间有三种关系。分别是：

* 你的眼里都是我，我却不一定只有你——包含关系。

* 你死我活——互斥关系。

* 百年好合——相等关系。

  <img src='https://upload-images.jianshu.io/upload_images/18367673-e6f59b7a3ad32454.png' />

事件的运算。以事件$A$和事件$B$为例。

1. 事件的并。

   $A$与$B$至少有一个事件发生了。记为$A\cup B$.

2. 事件的交。

   $A、B$均发生了。记为$A\cap B$，亦可记为$AB$.

3. 事件的差。

   若$A$发生而$B$不发生称为$A$与$B$的差。记为$A-B$。

4. 对立事件。

   $A$不发生的事件称为$A$的对立事件。记为$\bar{A}$。

   <img src='https://upload-images.jianshu.io/upload_images/18367673-32c204ad44df7161.png' height='50%' width='50%' />

## 1.2 事件的概率及性质

### 1.2.1 频率与概率

对于一个随机事件，如果我们想知道它发生的可能性大小，我们可以通过随机试验的方式来了解这个可能性。假设一个随机事件$A$在$n$次重复实验中出现了$n_A$次，我们就将比值$n_A/n$定义为$A$发生的频率，记为$f_n(A)$。
$$
f_n(A)=\frac{n_A}{n}
$$
例如我们做一个扔骰子的实验，进行$n$次实验，分别计算在实验过程中各个点数到目前为止出现的频率，绘制频率随试验次数变化的统计图如下，我们可以看到，随着试验次数的增加，各个点数出现的频率逐渐趋向于一个固定的值，当$n$趋向于无穷大时，各个点数的频率将趋近于$1/6$，我们就称之为概率。

某一事件$A$发生的概率记为$P(A)$。

从前面的韦恩图我们不难得到关于事件间的运算的概率的一些公式：
$$
P(A\cup B)=P(A)+P(B)-P(A\cap B)\\
P(A\cap B)=P(A)+P(B)-P(A\cup B)\\
P(\bar{A})=1-P(A)
$$

## 1.3 等可能概型（古典概型）

有一类随机试验有这样的特点：

1. 样本空间的元素个数为有限个。
2. 样本空间中每个基本事件（样本点）发生的可能性相同

这样的随机试验称为等可能概型，又叫古典概型。上一节中的扔骰子试验就是一个古典概型，首先样本空间只有六个元素，分别是六个点数，其次，六个点数出现的概率都相同。 

对于一个古典概型$E$，其样本空间有$n$个样本点，事件$A$有$m$个样本点，则事件$A$发生的概率为：
$$
P(A)=\frac{m}{n}=\frac{A包含的样本点数}{样本总点数}
$$
下面我们通过一些逐渐深入的例子理解一下古典概型。

1. 老师上课随机抽取一班学生回答问题，共有$n$个学生，需要抽取$m$个不同的学生回答问题，假如你在这个班上，那么你被抽中回答问题的概率是多大呢？

   分析：首先我们确定样本空间的样本数，也就是能够被老师抽中回答问题的所有可能的情况数目。也就是从$n$中抽取$m$个无序元素，共有$C_n^m$个组合。接下来我们来计算在这些组合中你被抽到的有多少次，既然$m$个人中已经确定有你，所以说，我们只需要从剩下的$n-1$个人中再抽取$m-1$个人即可，也就是$C_{n-1}^{m-1}$个组合，因此，你被抽到的概率为$\frac{C_{n-1}^{m-1}}{C_n^m}$。

2. 口袋中有$N$个球，其中黑球有$M$个，剩下为白球随机抽取$n$个，讨论在每次抽取一个之后再放回口袋中再抽下一个和抽取之后不再放回两种情况下，恰好抽取$k$个黑球的概率。

   分析：

   * 不放回：
     $$
     P(恰好抽出k个黑球)=\frac{黑球中抽出k个的组合数\times白球中抽出(n-k)个的组合数}{所有抽取情况的数目}\\
     =\frac{C_M^kC_{N-M}^{n-k}}{C_N^n}
     $$

   * 有放回：
     $$
     P(恰好抽出k个黑球)=\frac{n个位置中选出k个给黑球\times k个黑球的排列数目 \times (n-k)个白球的排列数目}{所有抽取情况的数目}\\
     =\frac{C_n^kM^k(N-M)^{n-k}}{N^n}=C_n^k(\frac{M}{N})^k(1-\frac{M}{N})^{n-k}
     $$

3. 二班共有$n$个学生，假设每个学生在$365$天中任意一天生日等可能，求有两个同学的生日同一天的概率。

   分析：这个问题我们可以从它的反面入手，即任意两个同学都不在一天生日，那么我们就要从$365$天选出$n$天作为有同学生日的日期，在这$n$天中，学生可以任意排列。
   $$
   P(\bar{A})=\frac{C_N^nA_n^n}{N^n}\\
   P(A)=1-P(\bar{A})=1-\frac{C_N^nA_n^n}{N^n}
   $$
   我们绘制一下随$n$变化该事件发生的概率变化图。

   



## 1.4 几何概型

古典概型要求样本空间中的样本点总数有限，然而在实际问题中，很多请情况下样本点有无限多，但是其仍然满足各个样本点具有等可能的特点，在这种情况下，我们可以利用几何方法计算概率。首先举一个最简单的例子，在区间$[0,1]$上任取一个数，求该数小于$0.5$的概率。此时我们便可以利用区间长度的度量来计算该概率，即：
$$
P(A)=\frac{区间[0,0.5]的宽度}{区间[0,1]的宽度}=\frac{1}{2}
$$

### 1.4.1蒲丰投针问题

平面上有等距离平行线，平行线之间的距离为$a$。向此平面投掷一根长为$l(l<a)$的针，求针与平行线相交的概率。

<img src='https://upload-images.jianshu.io/upload_images/18367673-7f98e7c615325a86.png' height='50%' width='50%' />

分析：假设$M$为针的中点，$M$到最近的平行线的距离为$x$针与平行线的夹角为$\theta$针的位置可以由$(x,\theta)$决定，可以得到样本空间为
$$
\Omega=\{(x,\theta)|0\leq x\leq \frac{a}{2},0\leq \theta \leq\pi\}
$$
针与平行线相交的充要条件为
$$
x\leq \frac{l}{2}sin\theta
$$
因此
$$
A=\left\{0\leq x\leq\frac{l}{2}sin\theta,0\leq\theta\leq\pi\right\}\\
\therefore P(A)=\frac{m(A)}{m(\Omega)}=\frac{\int_0^\pi\frac{l}{2}sin\theta d\theta}{a\frac{\pi}{2}}=\frac{2l}{a\pi}
$$



这个答案很有趣，因为它是和$\pi$有关系的，之前我们曾说过随着试验次数的不断增多，试验中某事件发生的频率将在概率上不断接近理论概率，利用这个原理和蒲丰投针试验，我们能够得到一种估计$\pi$的方法，即过重复投针来计算针与平行线相交的频率，并根据上式计算出$\pi$的估计值。历史上已经有学者做个这个实验，结果如下所示

|      试验者      | l/a  | 投掷次数 | 相交次数$m$ | $\pi$近似值 |
| :--------------: | :--: | :------: | :---------: | :---------: |
|   Wolf 1850年    | 0.8  |   5000   |    2532     |   3.1596    |
|   Smith 1855年   | 0.6  |   3204   |    1219     |   3.1541    |
| De Morgan 1860年 | 1.0  |   600    |     383     |   3.1332    |
|    Fox 1884年    | 0.75 |   1030   |     489     |   3.1595    |

有了计算机之后，我们进行该实验的效率要高很多，我们只需要随机产生中点到平行线的最短距离和夹角$\theta$即可。

### 1.4.2 蒙特卡罗方法

利用上一小节的模拟思路发展出了一门新的统计模拟方法：蒙特卡罗方法。下面我们通过例子来加深理解。

仍然是估计$\pi$，只不过我们用一种更直观的方法取估计。

<img src='https://upload-images.jianshu.io/upload_images/18367673-d5874d5cd7cb6fef.png' height='50%' width='50%'/>

如图所示，我们随机在正方形内取多个点，然后通过计算落在圆内的频率，用频率估计概率。
$$
P(点落在圆内)=\frac{S_圆}{S_{正方形}}=\frac{\pi r^2}{4r^2}=\frac{\pi}{4}
$$



## 1.5. 条件概率

### 1.5.1 条件概率

有的时候，某一个事件发生的概率往往会受到另一个事件的影响，例如，你每周坚持锻炼，得病的概率就会降低，这种利用新得到的信息重新考虑事件发生的概率就成为条件概率。在事件$B$发生的条件下事件$A$发生的概率就记作$P(A|B)$，上文中的例子就可以记作$P(得病|每周坚持锻炼)$。

一个古典概型的例子就是掷骰子，我们定义
$$
\begin{split}
&A:骰子的点数为6。\\
&B:骰子的点数大于3.
\end{split}
$$
不难得出，事件$A$发生的概率为$\frac{1}{6}$，事件$B$发生的概率为$\frac{1}{2}$，那么我们刚刚学到的条件概率如何运用呢？在$B$发生的条件下，$A$发生的概率为$A$的样本点的个数$B$的样本点的个数，也就是$\frac{1}{3}$。我们发现，在这个例子中事件$B$的发生让我们更加相信$A$会发生了，因为$A$发生的概率从$\frac{1}{6}$提高到了$\frac{1}{3}$我们可以总结如下。
$$
P(A|B)=\frac{AB的样本点个数}{B的样本点个数}=\frac{P(AB)}{P(B)}				\tag{1.5.1}
$$
虽然上式是从古典概型推出的结论，但是其对几何概型仍然适用。

**独立事件**

既然提到了一个事件对另一个事件有影响的情况，我们很有必要再去讨论一下没有影响的情况，我们仍然沿用刚刚的掷骰子试验，只是我们投掷两次，并且我们修改事件$A$和$B$如下。
$$
\begin{split}
&A:第一次掷骰子的点数为6。\\
&B:第二次骰子的点数大于3.
\end{split}
$$
现在我们再来看看这次会有什么样的奇妙的答案
$$
P(A)=\frac{1}{6}\\
P(A|B)=\frac{1}{6}
$$
我们发现，即使我们知道了$B$发生了，但是这一信息并未提供我们判断$A$发生的可能性的信息，仿佛听见了$A$在说，“你发生关我啥事儿？”。因此我们称这种满足
$$
P(A|B)=P(A)\\
P(B|A)=P(B)\\
$$
关系的事件为独立事件。

**乘法公式**

将公式$(1.5.1)$稍作变形即可得到计算事件的交的公式。
$$
P(AB)=P(A)P(B|A)\\
P(AB)=P(B)P(A|B)
$$
另外，如果我们知道了$A$和$B$之间是独立的，则可变形为
$$
P(AB)=P(A)P(B)
$$

## 1.6 贝叶斯定理

在之前我们讨论条件概率的时候，我们知道了，在获得了新信息之后，可以对某一事件发生的概率进行修正。一般来说，在数据分析开始的时候，我们对事件发生的概率有一个初始的先验概率，当我们获得了更多的数据或者分析资料之后，我们就可以根据这些资料修正我们关注的事件的概率，修正得到的概率就称为后验概率。贝叶斯定理就是这种概率计算的一种方法。下图就是概率修正的步骤。

<img src='https://upload-images.jianshu.io/upload_images/18367673-fa982474bd13be0e.jpg' height='60%' width='60%' align=center />

为了引入贝叶斯定理，我们考虑如下的例子。

有一种疾病的发病率是百分之一，医院有一种试验阶段的化验技术可以对这种疾病进行诊断，但是这种方法却不够精确，有百分之五的概率将患病诊断为正常人，百分之三的概率将正常人诊断为患者。现在假设一个人的化验结果显示为有病，仅根据这一化验结果推测，那么这个人确实患病和不患病的概率分别有多大？
$$
\begin{split}
&A_1:被诊断者患病。\\
&A_2:被诊断者不患病。\\
&B_1:诊断结果为患病。\\
&B_2:诊断结果为不患病。
\end{split}
$$

<img src='https://upload-images.jianshu.io/upload_images/18367673-f63b0d1ea5b61d7a.png' height='60%' width='60%' align=center />

接下来我们用乘法公式分别计算每一种实验结果出现的概率，我们只需要将每一步上的概率相乘即可。

<img src='https://upload-images.jianshu.io/upload_images/18367673-6f6b2a93fbf547aa.png' height='70%' width='70%' />

现在我们再来计算化验结果为有病的情况下这个人患病的概率，就可以直接根据概率图上的信息计算了。
$$
\begin{split}
P(A_1|B_1)=\frac{P(A_1B_1)}{P(B_1)}\\
P(A_2|B_1)=\frac{P(A_2B_1)}{P(B_1)}
\end{split}
\tag{1.6.1}
$$
从概率树中我们能看到，只有在两种情况下才会发生诊断结果为有病的情况：

1. 被诊断者患病, 即$(A_1, B_1)$。
2. 被诊断者不患病，即$(A_2, B_1)$。

因此有：
$$
\begin{split}
P(B_1)&=P(A_1B_1)+P(A_2B_1)\\
&=P(A_1)P(B_1|A_1)+P(A_2)P(B_1|A_2)
\end{split}
$$
再将上式代入到$(1.6.1)$，即可得到贝叶斯定理在两个事件下的情况：
$$
P(A_1|B_1)=\frac{P(A_1B_1)}{P(A_1)P(B_1|A_1)+P(A_2)P(B_1|A_2)}\\
P(A_2|B_1)=\frac{P(A_2B_1)}{P(A_1)P(B_1|A_1)+P(A_2)P(B_1|A_2)}
$$
代入数据即可：
$$
\begin{split}
P(A_1|B_1)&=\frac{P(A_1B_1)}{P(A_1)P(B_1|A_1)+P(A_2)P(B_1|A_2)}\\
&=\frac{0.0095}{0.01\times 0.95+0.99\times 0.03}\\
&=0.2423=24.23\%
\end{split}
\\
\begin{split}
P(A_2|B_1)&=\frac{P(A_2B_1)}{P(A_1)P(B_1|A_1)+P(A_2)P(B_1|A_2)}\\
&=\frac{0.0297}{0.01\times 0.95+0.99\times 0.03}\\
&=0.7577=75.77\%
\end{split}
$$
在进行诊断之前，我们只知道被诊断者患病的概率为$1\%$，而在诊断之后，我们就能了解到这个人患病的概率提高到了$24.3\%$。

贝叶斯定理通常用于以下情况，我们希望计算后验概率的那些事件是互斥的，他们的并构成了整个样本空间。推广$n$个事件的贝叶斯定理为：
$$
P(A_i|B)=\frac{P(A_i)P(B|A_i)}{\sum\limits _{i=1}^nP(A_i)P(B|A_i)}
$$


